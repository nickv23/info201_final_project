subset <- with(babynames, babynames[year == 1998 & sex == "F",] )
with(subset, name[which.max(n)]) # which.max()  position of the max value
# This is sort of a messy way, because you make unnessesary variable called "subset"
# most popular female baby name ins 1920
subset <- with(babynames, babynames[year == 1920 & sex == "F",] )
with(subset, name[which.max(n)])
head(select(babynames, name, year, sex))
install.packages("babynames")
library(babynames)
boys <- babynames[babynames$sex == "M",]
boys <- with(babynames, babynames[sex == "M",])
## which were the most common male, female baby bames in 1998
# filter out 1998 data
subset <- with(babynames, babynames[year == 1998 & sex == "F",] )
with(subset, name[which.max(n)]) # which.max()  position of the max value
# This is sort of a messy way, because you make unnessesary variable called "subset"
# most popular female baby name ins 1920
subset <- with(babynames, babynames[year == 1920 & sex == "F",] )
with(subset, name[which.max(n)])
head(select(babynames, name, year, sex))
a <- filter(babynames, sex == "M", year == 2005)
filter(a, min_rank(desc(n)) == 5)
## command shift m creates the pipe --> %>%
babynames %>%
select(-prop) %>%
filter(year == 199 & sex == "F") %>%
arrange(desc(n)) %>%
head()
a <- filter(babynames, sex == "M", year == 2005)
filter(a, min_rank(desc(n)) == 5)
## command shift m creates the pipe --> %>%
babynames %>%
select(-prop) %>%
filter(year == 199 & sex == "F") %>%
arrange(desc(n)) %>%
head()
babynames %>%
select(-prop) %>%
filter(year == 199 & sex == "F") %>%
arrange(desc(n)) %>%
head()
babynames %>% select(-prop) %>%
filter(year == 199 & sex == "F") %>%
arrange(desc(n)) %>%
head()
babynames %>% select(-prop) %>%
filter(year == 199 & sex == "F") %>%
arrange(desc(n)) %>%
head() %>%
.$name
install.packages("Titanic")
install.packages("titanic")
library(titanic)
view(Titanic)
View("titanic"")
view(titanic)
View(titanic)
View(Titanic)
## This is not a data frame. Use the `is.data.frame()` function to confirm this.
is.data.frame(Titanic)
## Be sure to **not** treat strings as factors!
titanic_dataframe <- data_frame(Titanic, stringAsFactors = FALSE)
install.packages("ggplot")
install.packages("ggplot2")
library(ggplot2)
head(economics)
head(diamonds)
ggplot(economics) + geom_line(aes(x=date, y=unempmed))
ggplot(economics) + geom_line(aes(x=date, y=uempmed))
ggplot(economics) + geom_point(aes(x=date, y=uempmed))
##
party <- c(rep("R", 10), rep("D", 20))
party
politics <- data.frame(party)
politics
ggplot(plitics) + gemo_bar(aes(party))
ggplot(politics) + gemo_bar(aes(party))
ggplot(politics) + geom_bar(aes(party))
ggplot(politics) + geom_bar(aes(x=party))
ggplot(diamonds) + gemo_histogram(aes(x=price))
ggplot(diamonds) + geom(aes(x=price))
ggplot(diamonds) + geom_histogram(aes(x=price))
ggplot(diamonds) + geom_histogram(aes(x=price), bins = 100)
fr <- map_data("France")
head(fr) %>%
knitr:kable()
library(dplyr)
fr <- map_data("France")
head(fr) %>%
knitr:kable()
fr <- map_data("France")
install.packages("maps")
library(maps)
fr <- map_data("France")
#
fr <- map_data("france")
head(fr)
library(ggplot2)
fr <- map_data("france")
View(fr)
ggplot(fr,aes(long, lat, group=group)) + geom_polygon()
ggplot(fr) +
geom_polygon(aes(x=long, y=lat, group=group))
ggplot(fr) +
geom_polygon(aes(x=long, y=lat, group=group, col="skyblue"))
# same thing as below
ggplot(fr) +
geom_polygon(aes(x=long, y=lat, group=group), col="skyblue"))
ggplot(fr) +
geom_polygon(aes(x=long, y=lat, group=group), col="skyblue")
setwd("Desktop/INFO 201/a6-server-johanlia000/")
getwd()
rm(list=ls())
getwd()
t.test(H)
H=c(1.2,0.9,0.7,1.0,1.7,1.7,1.1,0.9,1.7,1.9,1.3,2.1,
1.6,1.8,1.4,1.3,1.9,1.6,0.8,2.0,1.7,1.6,2.3,2.0)
P=c(1.6,1.5,1.1,2.1,1.5,1.3,1.0,2.6)
t.test(H)
t.test(P)
boxplot(H, P,names=c("high-quality","low-quality"))
t.test(H, P, alternative="two.sided")
t.test(H,P,alternative="less")
t.test(H,P,alternative="greater")
boxplot(H, P,names=c("high-quality","low-quality"))
t.test(H,P,alternative="two.sided")
t.test(H,P,paired=T, alternative="two.sided")
t.test(H,P,alternative="two.sided")
t.test(H,P,paired=T, alternative="two.sided")
H = H[1:8]                            # Keep only the 1st 8 cases in above H.
boxplot(H, P,names=c("high-quality","low-quality"))
t.test(H,P,alternative="two.sided")
t.test(H,P,paired=T, alternative="two.sided")
plot(H,P)
H=c(1.2,0.9,0.7,1.0,1.7,1.7,1.1,0.9,1.7,1.9,1.3,2.1,
1.6,1.8,1.4,1.3,1.9,1.6,0.8,2.0,1.7,1.6,2.3,2.0)
P=c(1.6,1.5,1.1,2.1,1.5,1.3,1.0,2.6)
t.test(H)
t.test(P)
boxplot(H, P,names=c("high-quality","low-quality"))
t.test(H, P, alternative="two.sided")
t.test(H,P,alternative="less")
t.test(H,P,alternative="greater")
H = H[1:8]
boxplot(H, P,names=c("high-quality","low-quality"))
t.test(H,P,alternative="two.sided")
turkey = c(0.98, 0.65, 1.25, 1.89, 0.65, 0.95, 1.26, 1.61, 0.98, 0.45)
ham = c(1.34, 0.68, 0.57, 1.12, 0.36, 1.34, 0.53, 1.66, 0.55, 0.39)
t.test(turkey, ham ,alternative="less")
t.test(ham, turkey ,alternative="less")
turkey = c(0.98, 0.65, 1.25, 1.89, 0.65, 0.95, 1.26, 1.61, 0.98, 0.45)
ham = c(1.34, 0.68, 0.57, 1.12, 0.36, 1.34, 0.53, 1.66, 0.55, 0.39)
t.test(turkey, ham ,alternative="less")
t.test(ham, turkety ,alternative="greater")
turkey = c(0.98, 0.65, 1.25, 1.89, 0.65, 0.95, 1.26, 1.61, 0.98, 0.45)
ham = c(1.34, 0.68, 0.57, 1.12, 0.36, 1.34, 0.53, 1.66, 0.55, 0.39)
t.test(ham, turkey ,alternative="greater")
t.test(turkey, ham, alternative="two.sided")
shiny::runApp('Desktop/INFO 201/a8-app-johanlia000')
runApp('Desktop/INFO 201/a8-app-johanlia000')
# Input the data:
x<-c(4.6, 17.0, 17.4, 18.0, 18.5, 22.4,26.5, 30.0, 34.0, 38.8, 48.2, 63.5, 65.8,
73.9, 77.2, 79.8, 84.0)
y<-c(0.66, 0.92, 1.45, 1.03, 0.7, .73, 1.20, 0.8, 0.91, 1.19, 1.15, 1.12,
1.37, 1.45, 1.50, 1.36, 1.29)
# plot the data
plot(x,y)
# Fit the linear regression model:
fit<-lm(y~x)
summary(fit)
# and, the std.error (which is s_b, using the notation from lecture slides)
# can be computed as s_e/(sqrt(s_xx), where the "_e" means subscript "e":
se<-0.2031   # s_e, or "residual standard error"
sxx<-sum((x-mean(x))^2) # s_xx
se/sqrt(sxx) # std.error
# We just used a t-test of the slope. Now, let's obtain the confidence interval for
# the slope:
confint(fit)
# Finally, let's perform a test of the null hypothesis that the population correlation between
# y and x1 is 0 versus the alternative that the population correlation is not 0.
cor.test(y,x)
# The sample correlation 0.7122403.
# The t-statistic is 3.9298 and the p-value is 0.001337. # Note that the
# t-statistic and p-value (rounded) are exactly the same as those from the
# t-test of the null hypothesis that the mean increase in y associated with
# a 1 unit increase in x is 0, in favor of the alternative that it is not 0.
# We can also do this "by hand" using the sample correlation and sample size,
# using the formulas in the text and from lecture:
rr<-cor(y, x)
nn<-length(y)
tobs<-rr*sqrt(nn-2)/sqrt(1-rr^2)
tobs
2*pt(tobs,df=nn-2,lower.tail=FALSE)
# Let's also provide an estimate and 95% confidence interval for the
# mean abrasive wear loss when austentite content is 40%
predict(fit,data.frame(x=40))
# We estimate the mean abrasive wear loss is 1.090014 mm^3 when austentite
# content is 40%. Look at the plot. This seems reasonable.
predict(fit,data.frame(x=40),interval="confidence")
# Now, let's predict an individual abrasive wear loss when austentite content
# is 40%
predict(fit,data.frame(x=40),interval="prediction")
# Let's also provide an estimate and 95% confidence interval for the
# mean abrasive wear loss when austentite content is 40%
predict(fit,data.frame(x=40))
x <- c(5, 12, 14, 17, 23, 30, 40, 47, 55, 67, 72, 81, 96, 112, 127)
y <- c(4, 10, 13, 15, 15, 25, 27, 46, 38, 46, 53, 70, 82, 99, 100)
# Plot in scatterplot form
plot(x, y)
sxy <- sum(x*y)
sxy
sum(x)
sum(y)
length(x)
sxy <- sum(x*y) - ((sum(x)*sum(y)) / length(x))
sxy
sum(x*x)
sxx <- sum(x*x) - (sum(x)^2 / length(x))
b <- sxy / sxx
b
a <- mean(y) - b * mean(x)
mean(x)
syy <- sum(y*y) - (sum(y)^2 / length(y))
std <- syy - b * sxy
syy <- sum(y*y) - (sum(y)^2 / length(y))
ssresid <- syy - b * sxy
std <- ssresid / (length(x) - 2)
std
std <- sqrt(variance)
variance <- ssresid / (length(x) - 2)
std <- sqrt(variance)
std
syy
ssto <- syy
ssto
syy
# Number 11.22
x2 <- c(.11, .13, .14, .18, .29, .44, .67, .78, .93)
y2 <- c(1.72, 2.17, 2.33, 3, 5.17, 7.61, 11.17, 12.72, 14.78)
length(x2) == length(yz)
length(x2) == length(y2)
sxy2 <- sum(x2 * y2) - ((sum(x2) * sum(y2))/ length(x2))
sxy2
sxx2 <- sum(x2 * x2) - (sum(x2)^2) / length(x2))
sxx2 <- sum(x2 * x2) - (sum(x2)^2 / length(x2))
sxx2
sxx
mean(y2)
lm(y2~x2)
sxx2 <- sum(x2 * x2) - (sum(x2)^2 / length(x2))
sxxtest<-sum((x2-mean(x2))^2)
sxx2
sxxtest
mean(x2)
sxy2 <- sum(x2*y2) - ((sum(x2)*sum(y2)) / length(x2))
sxx2 <- sum(x2 * x2) - (sum(x2)^2 / length(x2))
sxy2
sxx2
sxy2/sxx2
sxy2
sxx2
lm(y2~x2)
.00757/.001926
# We just used a t-test of the slope. Now, let's obtain the confidence interval for
# the slope:
confint(fit)
predict(fit,data.frame(x=.4),interval="confidence")
predict(fit,data.frame(x=.4),interval="prediction")
fit <- lm(y2~x2)
predict(fit,data.frame(x=.4),interval="confidence")
predict(fit,data.frame(x=.4),interval="prediction")
fit <- lm(y2~x2)
fit
predict(fit,data.frame(x=.4),interval="confidence")
summary(fit)
mean(x2)
fit
class(fit)
confit(fit)
confint(fit)
# We just used a t-test of the slope. Now, let's obtain the confidence interval for
# the slope:
confint(fit)
# Fit the linear regression model:
fit<-lm(y~x)
# We just used a t-test of the slope. Now, let's obtain the confidence interval for
# the slope:
confint(fit)
predict(fit,data.frame(x=.4),interval="confidence")
predict(fit,data.frame(x=.4),interval="prediction")
predict(fit2,data.frame(x=.4),interval="confidence")
fit2 <- lm(y2~x2)
confint(fit2)
predict(fit2,data.frame(x=.4),interval="confidence")
newData <- data.frame(x=.4)
newData
predict(fit2,newData,interval="confidence")
fit2
fit
data.frame(x=40)
predict(fit2, newData,interval="confidence")
newData <- data.frame(x2=.4)
predict(fit2, newData,interval="confidence")
predict(fit2,newData,interval="prediction")
obscounts<-matrix(c(59.5,53.3,56.8,63.1,58.7,
55.2,59.1,52.8,54.5,
51.7,48.8,53.9,49,
60.6,58.5,55,65.2,61.3),4,5, byrow=T)
dat = read.table("http://faculty.washington.edu/lynb/StatMath390/9_1_dat.txt",header=TRUE)
dat
# C7.1
EC1 <- c(59.5,53.3,56.8,63.1,58.7)
EC2 <- c(55.2,59.1,52.8,54.5)
EC3 <- c(51.7,48.8,53.9,49)
EC4 <- c(60.6,58.5,55,65.2,61.3)
Data <- data.frame(
Y=c(EC1, EC2, EC3, EC4),
Site =factor(rep(c("EC 1.6", "EC 3.8", "EC 6", "Ec 10.2"), times=c(length(EC1), length(EC2),
length(EC3), length(EC4))))
)
aD
Data
View(Data)
Data <- data.frame(
Y=c(EC1, EC2, EC3, EC4),
EC_Level =factor(rep(c("1.6", "3.8", "6", "10.2"), times=c(length(EC1), length(EC2),
length(EC3), length(EC4))))
)
View(Data)
View(dat)
Data <- data.frame(
EC_Level =factor(rep(c("1.6", "3.8", "6", "10.2"),
Tomato=c(EC1, EC2, EC3, EC4), times=c(length(EC1), length(EC2),
length(EC3), length(EC4))))
)
Data <- data.frame(
EC_Level =factor(rep(c("1.6", "3.8", "6", "10.2"),
Tomato =c(EC1, EC2, EC3, EC4), times=c(length(EC1), length(EC2), length(EC3), length(EC4))))
)
EC_Level <- c(1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4)
Tomato_Yield <- c(59.5,53.3,56.8,63.1,58.7,55.2,59.1,52.8,54.5, 51.7,48.8,53.9,49,51.7,48.8,53.9,49  )
length(EC_Level)
length(Tomato_Yield)
EC_Level <- c(1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4)
Tomato_Yield <- c(59.5, 53.3, 56.8, 63.1, 58.7,
55.2, 59.1, 52.8, 54.5,
51.7, 48.8, 53.9, 49,
60.6, 58.5, 55, 65.2, 61.3)
length(Tomato_Yield)
Data <- data.frame(EC_Level = EC_Level, Tomato_Yield = Tomato_Yield)
View(Data)
Data <- data.frame(EC_Level = EC_Level, Tomato_Yield = Tomato_Yield)
aov.ec <- aov(Tomato_Yield~ as.factor(EC_Level), data=Data)
summary(aov.ec)
dat = read.table("http://faculty.washington.edu/lynb/StatMath390/9_1_dat.txt",header=TRUE)
aov.1 = aov(Vibration~ as.factor(Brand), data=dat)
# Remember "as.factor" is important. It tells R to treat Brand as a categorical variable
# with categories 1, 2, ..., 5
# instead of as a continuous variable with values 1, ..., 5
# With as.factor(Brand), the F-test has 4 numerator degrees of freedom (for 5-1 categories)
summary(aov.1)  # you can compare this to Table 9.1 in the textbook, or to doing it by hand further below
Data <- data.frame(EC_Level = EC_Level, Tomato_Yield = Tomato_Yield)
aov.ec <- aov(Tomato_Yield~ as.factor(EC_Level), data=Data)
summary(aov.ec)
# Part b
lin =lm(Tomato_Yield~factor(EC_Level), data=Data)
summary(lin)
# Part b
lin =lm(Tomato_Yield~as.factor(EC_Level), data=Data)
summary(lin)
ex.iron<-c(61,175,111,124,130,173,169,169,160,244,257,333,199)
ex.al<-c(13,21,24,23,64,38,33,61,39,71,112,88,54)
phos.adsorp<-c(4,18,14,18,26,26,21,30,28,36,65,62,40)
# plot the data
plot(data.frame(ex.iron,ex.al,phos.adsorp))
# fit the multiple linear regression model
fit<-lm(phos.adsorp~ex.iron+ex.al)
summary(fit)
# y = tomato yield
# x = EC level
plot(EC_Level, Tomato_Yield)
# C7.1
## part a
EC_Level <- c(1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4)
Tomato_Yield <- c(59.5, 53.3, 56.8, 63.1, 58.7,
55.2, 59.1, 52.8, 54.5,
51.7, 48.8, 53.9, 49,
60.6, 58.5, 55, 65.2, 61.3)
Data <- data.frame(EC_Level = EC_Level, Tomato_Yield = Tomato_Yield)
View(Data)
sim_lin <- lm(Tomato_Yield~as.factor(EC_Level))
summary(sim_lin)
# Input the data:
x<-c(4.6, 17.0, 17.4, 18.0, 18.5, 22.4,26.5, 30.0, 34.0, 38.8, 48.2, 63.5, 65.8,
73.9, 77.2, 79.8, 84.0)
y<-c(0.66, 0.92, 1.45, 1.03, 0.7, .73, 1.20, 0.8, 0.91, 1.19, 1.15, 1.12,
1.37, 1.45, 1.50, 1.36, 1.29)
# plot the data
plot(x,y)
# Fit the linear regression model:
fit<-lm(y~x)
summary(fit)
# y = tomato yield
# x = EC level
plot(EC_Level, Tomato_Yield)
sim_lin <- lm(Tomato_Yield~EC_Level)
summary(sim_lin)
sim_lin <- lm(Tomato_Yield~EC_Level)
summary(sim_lin)
# Input the data:
x<-c(4.6, 17.0, 17.4, 18.0, 18.5, 22.4,26.5, 30.0, 34.0, 38.8, 48.2, 63.5, 65.8,
73.9, 77.2, 79.8, 84.0)
y<-c(0.66, 0.92, 1.45, 1.03, 0.7, .73, 1.20, 0.8, 0.91, 1.19, 1.15, 1.12,
1.37, 1.45, 1.50, 1.36, 1.29)
# plot the data
plot(x,y)
# Fit the linear regression model:
fit<-lm(y~x)
summary(fit) # FIND MEAN INCREASE
confint(fit) # USE TO FIND CONFIDENCE INTERVAL
cor.test(y,x)  # POPULATION CORRELATION IS 0 OR NOT
summary(fit) # FIND MEAN INCREASE
# Let's also provide an estimate and 95% confidence interval for the
# mean abrasive wear loss when austentite content is 40%
predict(fit,data.frame(x=40))
# We estimate the mean abrasive wear loss is 1.090014 mm^3 when austentite
# content is 40%. Look at the plot. This seems reasonable.
predict(fit,data.frame(x=40),interval="confidence")
# Now, let's predict an individual abrasive wear loss when austentite content
# is 40%
predict(fit,data.frame(x=40),interval="prediction")
ex.iron<-c(61,175,111,124,130,173,169,169,160,244,257,333,199)
ex.al<-c(13,21,24,23,64,38,33,61,39,71,112,88,54)
phos.adsorp<-c(4,18,14,18,26,26,21,30,28,36,65,62,40)
# plot the data
plot(data.frame(ex.iron,ex.al,phos.adsorp))
# fit the multiple linear regression model
fit<-lm(phos.adsorp~ex.iron+ex.al)
summary(fit)
# or by using the confint function
confint(fit)    # (0.0465, 0.179)
# Below we input the data and regression y on x1, x2, x3, and x4:
press<-c(1.4,2.2,4.6,4.9,4.6,4.7,4.6,4.5,4.8,1.4,4.7,1.6,4.5,4.7,4.8,4.6,4.3,4.9,1.7,4.6,2.6,3.1,4.7,2.5,4.5,2.1,1.8,1.5,1.3,4.6)
formaldehyde<-c(8,2,7,10,7,7,7,5,4,5,8,2,4,6,10,4,4,10,5,8,10,2,6,7,5,8,4,6,4,7)
catalyst<-c(4,4,4,7,4,7,13,4,7,1,10,4,10,7,13,10,13,10,4,13,1,13,13,1,13,1,1,1,1,10)
temp<-c(100,180,180,120,180,180,140,160,140,100,140,100,180,120,180,160,100,120,100,140,180,140,180,120,140,160,180,160,100,100)
time<-c(1,7,1,5,5,1,1,7,3,7,3,3,3,7,3,5,7,7,1,1,1,1,7,7,1,7,7,1,1,7)
# plot the data
plot(data.frame(press,formaldehyde,catalyst,temp,time))
# We first fit the "full" model, the model with all 4 predictors.
fit.full<-lm(press~catalyst+temp+time+formaldehyde)
summary(fit.full)
# Then, we fit the "reduced" model, the model without time and formaldehyde.
fit.reduced<-lm(press~catalyst+temp)
summary(fit.reduced)
# Then, we fit the "reduced" model, the model without time and formaldehyde.
fit.reduced<-lm(press~catalyst+temp)
summary(fit.reduced)
# We first fit the "full" model, the model with all 4 predictors.
fit.full<-lm(press~catalyst+temp+time+formaldehyde)
summary(fit.full)
length(time)
# Input the data again
ex.iron<-c(61,175,111,124,130,173,169,169,160,244,257,333,199)
ex.al<-c(13,21,24,23,64,38,33,61,39,71,112,88,54)
phos.adsorp<-c(4,18,14,18,26,26,21,30,28,36,65,62,40)
# plot the data again
plot(data.frame(ex.iron,ex.al,phos.adsorp))
# fit the reduced model
fit.reduced<-lm(phos.adsorp~1)
summary(fit.reduced)
# fit the full model
fit.full<-lm(phos.adsorp~ex.iron+ex.al)
summary(fit.full) # notice the F-statistic for model utility (92.03)
# do the F-test for subset of predictors
SSEreduced<-17.61^2*(13-1)
SSEfull<-4.379^2*(13-3)
( (SSEreduced-SSEfull)/2 ) / (SSEfull/(13-3))
pf(92.03,2,10,lower.tail=F)
anova(fit.reduced,fit.full) # notice the F-statistic (92.03)
###############################################################################
##### Effect of collinearity in multiple linear regression
# Back in lab 4, we examined the effect of collinearity on the regression
# coefficients, using fake/simulated data.
# Recall the make.fit function from lab 4. It is modified here to report the
# standard error of one of the coefficients from a multiple linear regression
# model with 2 predictors, x1 and x2, when the correlation (r) between x1 and
# x2 is varied.
make.fit = function(r)  # This function takes in r (i.e. cor between x1, x2,
{                       # not between y and anything.)
library(MASS)
set.seed(1)
n=100
dat =  mvrnorm(n, rep(0, 2), matrix(c(1,r,r,1),2,2))
x1 = dat[,1] ; x2 = dat[,2]
beta1 = 2
beta2 = 3
y = 1 + beta1*x1 + beta2*x2 + rnorm(n,0,0.1)  # Make y data, and add noise.
lm.1 = lm( y ~ x1 + x2)               # Here we fit a plain through the data.
return(summary(lm.1)$coeff[2,2])      # return the std err of beta 1.
}
r = stderrbeta1 = numeric(99)        # Make a 99-dimensional array for storing r and
# standard error of the estimate of beta1 (stderrbeta1)
for(i in c(1:99)){         # Run make.fit() 99 times.
r[i] = i/100
stderrbeta1[i] = make.fit(r[i])
}
plot(r,stderrbeta1)
setwd("Desktop/INFO 201/info201_final_project/")
shiny::runApp('~/Desktop/testerFinalProj')
runApp('~/Desktop/testerFinalProj')
runApp('~/Desktop/testerFinalProj')
